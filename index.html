<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Neil Gautam | AI Researcher</title>
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <nav class="top-nav">
    <div class="nav-container">
      <div class="nav-brand">Neil Gautam</div>
      <div class="nav-items">
        <a href="#summary" class="nav-item active">HOME</a>
        <a href="#about" class="nav-item">PROFILE</a>
        <a href="#experience" class="nav-item">EXPERIENCE</a>
        <a href="#projects" class="nav-item">RESEARCH</a>
        <a href="#publications" class="nav-item">PUBLICATIONS</a>
        <a href="#contact" class="nav-item">CONTACT</a>
      </div>
    </div>
  </nav>

  <main>
    <!-- SUMMARY -->
    <section id="summary" class="section hero-section">
      <div class="container summary-flex">
        <div>
          <span class="block-label">$ whoami</span>
          <h1>Neil Gautam</h1>
          <div class="subtitle">AI Researcher</div>
          <div class="topic-tags">Embodied AI &bullet; Human-Centric Computer Vision &bullet; Egocentric Systems</div>
          <div class="about-research">
            <span class="block-label">$ cat research_focus.txt</span>
            <div class="box research-summary">
              <p>
                My research interest lies at the intersection of embodied AI and human-centric computer vision, where I develop intelligent systems that can understand and predict how humans perceive, navigate, and interact with complex 3D environments. Drawing from my experience building production-scale vision systems and generative models, I've evolved toward addressing fundamental questions about human behavior prediction and spatial scene understanding.
              </p>
              <p>
                My current focus centers on egocentric vision and affordance modeling—developing frameworks that move beyond traditional computer vision to predict how humans will interact with their environment over time, bridging 2D visual perception with 3D spatial reasoning through advanced generative architectures, particularly diffusion models applied across diverse domains from healthcare to robust vision systems.
              </p>
              <p>
                What drives my research is the conviction that truly intelligent AI must understand the intentionality behind human actions, not just recognize visual patterns. My interdisciplinary approach combines computer vision, generative AI, and biomedical applications, positioning me to tackle fundamental challenges in embodied AI where spatial-temporal understanding of human behavior is critical. Whether developing memory-based architectures for anomaly detection, real-time monitoring systems that construct 3D scene representations, or geometry-aware reconstruction models for medical imaging analysis, I'm consistently working toward AI systems that don't just process visual data, but comprehend the physics, intentionality, and temporal dynamics of how humans move through and interact with their world—creating AI that truly understands human behavior in 3D space.
              </p>
            </div>
          </div>
        </div>
        <div class="metrics-col">
          <div class="metrics-card box">
            <div>
              <span class="metric-value">4.0</span>
              <span class="metric-label">GPA</span>
            </div>
            <div>
              <span class="metric-value">3</span>
              <span class="metric-label">Publications</span>
            </div>
            <div>
              <span class="metric-value">6+</span>
              <span class="metric-label">Research Projects</span>
            </div>
            <div>
              <span class="metric-value">3</span>
              <span class="metric-label">Years Exp</span>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- PROFILE / ABOUT -->
    <section id="about" class="section light-section">
      <div class="container profile-grid">
        <div class="card about-card">
          <h2 class="section-title">Profile & Education</h2>
          <div class="profile-info">
            <h3>Education</h3>
            <div class="profile-ed">
              <strong>Texas A&amp;M University</strong> <span class="muted">&bull; College Station, TX</span><br>
              <span>Master of Science in Computer Science (Aug 2024 - Jun 2026, Expected)</span><br>
              <strong>GPA:</strong> 4.0/4.0
              <div><strong>Advisors:</strong> 
                <a href="https://czhang0528.github.io/" target="_blank">Prof. Cheng Zhang</a>, 
                <a href="https://engineering.tamu.edu/biomedical/profiles/avazmohammadi-reza.html" target="_blank">Dr. Reza Avazmohammadi</a>
              </div>
              <div><strong>Coursework:</strong> Vision Foundation Models, ML for 3D Visualization & Graphics, Generative AI, Deep Reinforcement Learning, Research</div>
            </div>
            <div class="profile-ed">
              <strong>Delhi Technological University</strong> <span class="muted">&bull; Delhi, India</span><br>
              B.Tech in Information Technology (Aug 2018 - May 2022) <br>
              <strong>GPA:</strong> 8.6/10.0
            </div>
          </div>
        </div>
        <div class="card about-card">
          <h3>Skills Overview</h3>
          <div>
            <span class="list-label">Domains:</span> Embodied AI, Egocentric Vision, Human-Centric CV, 3D Scene Understanding, Diffusion Models, Medical Imaging, 3D Gaussian Splatting, Multi-view Tracking <br>
            <span class="list-label">Frameworks:</span> PyTorch, TensorFlow, Hugging Face, Keras, Open3D, Pyvista, Pandas, Matplotlib <br>
            <span class="list-label">Programming:</span> Python, C++, JavaScript, Ruby, SQL, Rust, Matlab<br>
            <span class="list-label">Cloud / MLOps:</span> AWS, GCP, Azure, Docker, Git <br>
            <span class="list-label">Vision/NLP:</span> OpenCV, CNN, RNN, LLAMA-2, Alpaca, BERT, RoBERTa, Transformers <br>
            <span class="list-label">Other:</span> Retriever Augmented Generation, 3D Gaussian Splatting, NeRF
          </div>
        </div>
      </div>
    </section>

    <!-- PROFESSIONAL EXPERIENCE -->
    <section id="experience" class="section">
      <div class="container">
        <h2 class="section-title">Professional Experience</h2>
        <div class="vertical-list">
          <div class="card exp-card">
            <div class="exp-header"><strong>Research Intern, AI4Arch</strong> <span class="muted">Texas A&M University, Jun 2025 – Aug 2025</span></div>
            <div class="exp-detail">Advisors: Dr. Cheng Zhang, Dr. Roxana Jafari &bull; Technologies: PyTorch, YOLOv11, ONNX, SMPL</div>
            <ul>
              <li>Developed multi-view detection (YOLOv11) & tracking algorithms for real-time people monitoring in intensive healthcare.</li>
              <li>Constructed observed 3D Scene (SMPL) with multi-view input for compliance monitoring.</li>
            </ul>
          </div>
          <div class="card exp-card">
            <div class="exp-header"><strong>Student Assistant, C2B Lab</strong> <span class="muted">Texas A&M University, Sep 2024 – Present</span></div>
            <div class="exp-detail">Advisor: Dr. Reza Avazmohammadi &bull; PyTorch, Matlab, Python, Viser</div>
            <ul>
              <li>Research on 4D heart motion tracking (2D→3D), diffusion-based registration, geometry-aware 3D cardiac mesh.</li>
            </ul>
          </div>
          <div class="card exp-card">
            <div class="exp-header"><strong>Machine Learning Engineer-2, AfterShoot</strong><span class="muted">Nov 2023 – Jul 2024, Delhi, India</span></div>
            <div class="exp-detail">Python, C++, Rust, RunPod, GCP, Azure, PyTorch, TensorFlow</div>
            <ul>
              <li>Spearheaded AI portrait enhancement and retouching (teeth correction, skin retouching), reducing editing from 10s of hours to &lt;3 hours/1000 photos.</li>
              <li>Engineered U-Net generative model for teeth correction, automated Lightroom pipelines, accelerated workflow by 40%.</li>
            </ul>
          </div>
          <div class="card exp-card">
            <div class="exp-header"><strong>Machine Learning Engineer, Savart</strong><span class="muted">Jan 2023 – Oct 2023, Hyderabad, India</span></div>
            <div class="exp-detail">Python, C++, AWS, GCP, Azure, PyTorch</div>
            <ul>
              <li>Fine-tuned LLAMA-2 / Alpaca on financial docs; boosted equity research from weeks to days.</li>
              <li>Engineered OCR/information extraction for 50K+ documents; boosted QA accuracy by 35% via RoBERTa/BART/XLM.</li>
            </ul>
          </div>
          <div class="card exp-card">
            <div class="exp-header"><strong>Deep Learning Intern, HyperVerge Ltd</strong> <span class="muted">Jul 2022 – Dec 2022, Bangalore, India</span></div>
            <div class="exp-detail">Supervisor: <a href="https://www.linkedin.com/in/hariprasad-p-s-162b0778/" target="_blank">Hariprasad P S</a></div>
            <ul>
              <li>Developed industry-leading ID tampering detection, serving 100+ clients and 15M+ requests/month.</li>
              <li>Pioneered ATAC-Net, achieving 98% catch rate &lt;1% false positive; ICIP2024 spotlight (top 5%).</li>
            </ul>
          </div>
        </div>
      </div>
    </section>

    <!-- RESEARCH PROJECTS -->
    <section id="projects" class="section light-section">
      <div class="container">
        <h2 class="section-title">Research Projects</h2>
        <div class="vertical-list">
          <div class="card proj-card">
            <div class="proj-header"><strong>Egocentric Fine-Grained Object Interaction Trajectory &amp; Affordance Prediction</strong></div>
            <span class="muted">May 2025 – Present | Advisor: <a href="https://czhang0528.github.io/">Prof. Cheng Zhang</a></span>
            <div class="proj-desc">End-to-end egocentric framework for 3D future location & fine-grained affordance interaction prediction in complex scenes.</div>
            <div class="tags">Egocentric Vision, Affordance Prediction, 3D Scene Understanding, Long-horizon Planning</div>
          </div>
          <div class="card proj-card">
            <div class="proj-header"><strong>HG-SCRUB: Human-Gaussian Scene Representation with Unified Baking</strong></div>
            <span class="muted">Jan 2025 – May 2025</span>
            <div class="proj-desc">Scene decomposition from sparse videos, learning material/physicals (Normals, Albedo etc.) with explicit depth/lighting for relightable human/scene baking.</div>
            <div class="tags">3D Gaussian Splatting, Material Decomposition, Neural Rendering, Human-Scene Separation</div>
          </div>
          <div class="card proj-card">
            <div class="proj-header"><strong>Free-Generation: Training-Free Video Diffusion Control</strong></div>
            <span class="muted">Aug 2024 – Dec 2024 | Advisor: <a href="https://czhang0528.github.io/">Prof. Cheng Zhang</a></span>
            <div class="proj-desc">Empirical work on training-free control of video diffusion models, introducing normalization/statistical techniques for enhanced control.</div>
            <div class="tags">Video Diffusion, Trajectory Control, Generative Models</div>
          </div>
        </div>
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section id="publications" class="section">
      <div class="container">
        <h2 class="section-title">Publications</h2>
        <div class="vertical-list">
          <div class="card pub-card">
            <div class="pub-title"><b>ATAC-Net: Zoomed view works better for Anomaly Detection</b></div>
            <div class="pub-meta">
              ICIP 2024 | Shaurya Gupta, <b>Neil Gautam</b>, Anurag Malyala
              <span class="muted">[<a href="https://ieeexplore.ieee.org/abstract/document/10647702" target="_blank">Paper</a>] [<a href="#" target="_blank">Code</a>]</span>
            </div>
            <div class="pub-desc">Memory-based semi-supervised architecture (ATAC-Net) achieving <b>98% catch rate</b> &lt;1% false positive. ICIP 2024 spotlight (top 5%).</div>
          </div>
          <div class="card pub-card">
            <div class="pub-title"><b>Obscenity Detection in Videos through Sequential ConvNet Pipeline Classifier</b></div>
            <div class="pub-meta">
              IEEE TCDS | <b>Neil Gautam</b>, Dr. Dinesh Kumar Vishwakarma
              <span class="muted">[<a href="https://ieeexplore.ieee.org/abstract/document/9733936" target="_blank">Paper</a>]</span>
            </div>
            <div class="pub-desc">ConvNet pipeline for video content moderation using spatial-temporal features for real-time analysis.</div>
          </div>
          <div class="card pub-card">
            <div class="pub-title"><b>Ensemble Learning Using Vision Transformer and CNN for Person ReID</b></div>
            <div class="pub-meta">
              IEEE ICCMC | Gupta, <b>Neil Gautam</b>, Dr. Vishwakarma
              <span class="muted">[<a href="https://ieeexplore.ieee.org/abstract/document/9753761" target="_blank">Paper</a>]</span>
            </div>
            <div class="pub-desc">Ensemble with Vision Transformer & CNNs for improved person re-ID across challenging datasets.</div>
          </div>
        </div>
      </div>
    </section>

    <!-- CONTACT / LINKS -->
    <section id="contact" class="section light-section">
      <div class="container">
        <h2 class="section-title">Contact & Links</h2>
        <div class="contact-list">
          <div><strong>Email:</strong> <a href="mailto:neilgautam.24@tamu.edu">neilgautam.24@tamu.edu</a></div>
          <div><strong>Phone:</strong> <a href="tel:+19797213148">+1 (979) 721-3148</a></div>
          <div><strong>Location:</strong> College Station, TX</div>
          <div><strong>LinkedIn:</strong> <a href="https://www.linkedin.com/in/neil-gautam-a928971a5/" target="_blank">linkedin.com/in/neil-gautam-a928971a5/</a></div>
          <div><strong>Github:</strong> <a href="https://github.com/neilgautam24-tamu" target="_blank">github.com/neilgautam24-tamu</a></div>
          <div><strong>Google Scholar:</strong> <a href="https://scholar.google.com/citations?user=CUISVw8AAAAJ&hl=en" target="_blank">Google Scholar</a></div>
          <div><strong>Key Mentors/Collaborators:</strong> 
            <a href="https://czhang0528.github.io/" target="_blank">Cheng Zhang</a> | 
            <a href="https://engineering.tamu.edu/biomedical/profiles/avazmohammadi-reza.html" target="_blank">Reza Avazmohammadi</a> | 
            <a href="https://www.linkedin.com/in/hariprasad-p-s-162b0778/" target="_blank">Hariprasad P S</a> | 
            <a href="https://www.linkedin.com/in/nikhil-bartwal-b07b501a3/" target="_blank">Nikhil Bartwal</a> | 
            <a href="https://www.linkedin.com/in/suhruth-eedara-93858412b/" target="_blank">Suhruth Eedara</a>
          </div>
        </div>
      </div>
    </section>
  </main>
</body>
</html>
