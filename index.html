<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Neil Gautam | AI Researcher</title>
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <nav class="top-nav">
    <div class="nav-container">
      <div class="nav-brand">neil_gautam@ai_researcher</div>
      <div class="nav-items">
        <a href="#hero" class="nav-item active">INIT</a>
        <a href="#profile" class="nav-item">PROFILE</a>
        <a href="#research" class="nav-item">RESEARCH</a>
        <a href="#publications" class="nav-item">PAPERS</a>
        <a href="#experience" class="nav-item">EXPERIENCE</a>
      </div>
    </div>
  </nav>
<main>
  <!-- HERO / SUMMARY -->
  <section id="hero" class="section wide-section">
    <div class="container hero-flex">
      <div class="hero-text">
        <span class="block-label">$ whoami</span>
        <h1>Neil Gautam</h1>
        <div class="subtitle">AI Researcher</div>
        <div class="topic-tags">Embodied AI &bullet; Human-Centric Computer Vision &bullet; Egocentric Systems</div>
        <span class="block-label">$ cat research_focus.txt</span>
        <div class="box research-summary">
          <p>My research interest lies at the intersection of embodied AI and human-centric computer vision, where I develop intelligent systems that can understand and predict how humans perceive, navigate, and interact with complex 3D environments. Drawing from my experience building production-scale vision systems and generative models, I've evolved toward addressing fundamental questions about human behavior prediction and spatial scene understanding.</p>
          <p>My current focus centers on egocentric vision and affordance modeling—developing frameworks that move beyond traditional computer vision to predict how humans will interact with their environment over time, bridging 2D visual perception with 3D spatial reasoning through advanced generative architectures, particularly diffusion models applied across diverse domains from healthcare to robust vision systems.</p>
          <p>What drives my research is the conviction that truly intelligent AI must understand the intentionality behind human actions, not just recognize visual patterns. My interdisciplinary approach combines computer vision, generative AI, and biomedical applications, positioning me to tackle fundamental challenges in embodied AI where spatial-temporal understanding of human behavior is critical. Whether developing memory-based architectures for anomaly detection, real-time monitoring systems that construct 3D scene representations, or geometry-aware reconstruction models for medical imaging analysis, I'm consistently working toward AI systems that don't just process visual data, but comprehend the physics, intentionality, and temporal dynamics of how humans move through and interact with their world—creating AI that truly understands human behavior in 3D space.</p>
        </div>
      </div>
      <div class="metrics-col">
        <div class="metrics-card box">
          <div>
            <span class="metric-value">4.0</span>
            <span class="metric-label">GPA</span>
          </div>
          <div>
            <span class="metric-value">3</span>
            <span class="metric-label">Publications</span>
          </div>
          <div>
            <span class="metric-value">6+</span>
            <span class="metric-label">Research Projects</span>
          </div>
          <div>
            <span class="metric-value">3</span>
            <span class="metric-label">Years Exp</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- EDUCATION & SKILLS -->
  <section id="profile" class="section light-section">
    <div class="container profile-columns">
      <div class="box split-card edu-card">
        <div>
          <h2 class="section-title">Education</h2>
          <h3>Master of Science in Computer Science</h3>
          <div class="edu-meta">Texas A&#38;M University <span class="small-sep">&bull;</span> Aug 2024 - Jun 2026 (Expected)</div>
          <div class="edu-meta">GPA: <b>4.0</b></div>
          <div class="edu-meta">Advisors: Prof. Cheng Zhang, Dr. Reza Avazmohammadi</div>
          <div class="edu-courses">
            <span class="list-label">Current Coursework:</span>
            Deep Reinforcement Learning, Generative AI, Research
            <br>
            <span class="list-label">Completed:</span>
            Vision Foundation Models, ML for 3D Visualization &#38; Graphics, Machine Learning, Analysis of Algorithms
          </div>
        </div>
      </div>
      <div class="box split-card skills-card">
        <h2 class="section-title">Skills</h2>
        <div>
          <span class="list-label">Domains:</span> Embodied AI, Egocentric Vision, Human-Centric CV, 3D Scene Understanding, Diffusion Models, Medical Imaging, 3D Gaussian Splatting, Multi-view Tracking
        </div>
        <div>
          <span class="list-label">Frameworks:</span> PyTorch, TensorFlow, Hugging Face
        </div>
        <div>
          <span class="list-label">Programming:</span> Python, C++, Rust
        </div>
        <div>
          <span class="list-label">Tools:</span> YOLO, ONNX, Open3D, Viser, Rerun.io, SMPL, OpenCV, Docker
        </div>
      </div>
    </div>
  </section>

  <!-- RESEARCH -->
  <section id="research" class="section">
    <div class="container">
      <h2 class="section-title">Research projects</h2>
      <div class="flex-cards">
        <div class="box">
          <div class="proj-header">
            <h3>Egocentric Fine-Grained Object Interaction Trajectory &amp; Affordance Prediction</h3>
            <span class="date-label">May 2025 – Present | Advisor: Prof. Cheng Zhang</span>
          </div>
          <div class="desc">Developing egocentric view driven end-to-end framework for predicting temporal future location prediction in 3D voxel scene representation along with precise & fine-grained affordance trajectory to interact with objects. This research enables long-horizon task planning in complex 3D environments.</div>
          <div class="tags">Egocentric Vision, Affordance Prediction, 3D Scene Understanding, Long-horizon Planning</div>
        </div>
        <div class="box">
          <div class="proj-header">
            <h3>HG-SCRUB: Human-Gaussian Scene Representation with Unified Baking</h3>
            <span class="date-label">Jan 2025 – May 2025</span>
          </div>
          <div class="desc">Developed framework for human and scene decomposition in separate representations given sparse image frames from video. Successfully learned material and physical components (Normals, Albedo, Roughness & Metallic) with explicit normal learning using depth derivation and baking for human gaussians to be relighted based on scene's environment map.</div>
          <div class="tags">3D Gaussian Splatting, Material Decomposition, Neural Rendering, Human-Scene Separation</div>
        </div>
      </div>
    </div>
  </section>

  <!-- PUBLICATIONS -->
  <section id="publications" class="section light-section">
    <div class="container">
      <h2 class="section-title">Publications</h2>
      <div class="flex-cards">
        <div class="box publication-card">
          <div class="paper-title"><strong>ATAC-Net: Zoomed view works better for Anomaly Detection</strong></div>
          <div class="paper-meta">ICIP 2024 | Shaurya Gupta, <b>Neil Gautam</b>, Anurag Malyala</div>
          <div class="paper-desc">Memory-based semi-supervised architecture achieving <b>98% catch rate</b> &lt;1% false positive through attention-guided cropping. <span class="tag-green">Spotlight (top 5%)</span>.</div>
          <div><a href="#">Paper</a> &bull; <a href="#">Code</a></div>
        </div>
        <div class="box publication-card">
          <div class="paper-title"><strong>Obscenity Detection in Videos through Sequential ConvNet Pipeline Classifier</strong></div>
          <div class="paper-meta">IEEE TCDS | <b>Neil Gautam</b>, Dr. Dinesh Kumar Vishwakarma</div>
          <div class="paper-desc">Sequential ConvNet pipeline for automated video content moderation using temporal-spatial feature extraction and classification methodologies for real-time video analysis.</div>
          <div><a href="#">Paper</a></div>
        </div>
      </div>
    </div>
  </section>

  <!-- EXPERIENCE -->
  <section id="experience" class="section">
    <div class="container">
      <h2 class="section-title">Experience</h2>
      <div class="flex-cards">
        <div class="box">
          <div class="exp-header"><h3>Research Intern, AI4Arch</h3>
            <span class="date-label">Texas A&amp;M University, June 2025 - Aug 2025</span>
          </div>
          <div class="exp-desc">Advanced multi-view detection and tracking systems for real-time healthcare monitoring. Pioneered people tracking and re-ID in intensive health units.</div>
          <ul class="exp-list">
            <li>Multi-view detection &amp; tracking in real-time with PyTorch, YOLOv11, ONNX, SMPL</li>
            <li>Constructed observed 3D Scene with objects using multi-view input</li>
            <li>Automated health code compliance monitoring</li>
          </ul>
        </div>
        <!-- Add other experience items here in same style -->
      </div>
    </div>
  </section>
</main>
</body>
</html>
