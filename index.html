<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Neil Gautam | AI Researcher</title>
  <link rel="stylesheet" href="styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <nav class="top-nav">
    <div class="nav-container">
      <div class="nav-brand">neil_gautam@ai_researcher</div>
      <div class="nav-items">
        <a href="#hero" class="nav-item active">INIT</a>
        <a href="#about" class="nav-item">PROFILE</a>
        <a href="#research" class="nav-item">RESEARCH</a>
        <a href="#publications" class="nav-item">PAPERS</a>
        <a href="#experience" class="nav-item">EXPERIENCE</a>
      </div>
    </div>
  </nav>
  
  <main>
    <!-- HERO -->
    <section id="hero" class="section hero-section">
      <div class="container hero-container card">
        <div>
          <span class="whoami-label">$ whoami</span>
          <h1 class="display-name">NEIL GAUTAM</h1>
          <div class="role-title">AI Researcher</div>
          <div class="specialization">[ Embodied AI • Human-Centric Computer Vision • Egocentric Systems ]</div>
          <div class="whoami-label">$ cat research_focus.txt</div>
          <div class="research-focus-block">
            <p>My research interest lies at the intersection of embodied AI and human-centric computer vision, where I develop intelligent systems that can understand and predict how humans perceive, navigate, and interact with complex 3D environments. Drawing from my experience building production-scale vision systems and generative models, I've evolved toward addressing fundamental questions about human behavior prediction and spatial scene understanding.</p>
            <p>My current focus centers on egocentric vision and affordance modeling—developing frameworks that move beyond traditional computer vision to predict how humans will interact with their environment over time, bridging 2D visual perception with 3D spatial reasoning through advanced generative architectures, particularly diffusion models applied across diverse domains from healthcare to robust vision systems.</p>
            <p>What drives my research is the conviction that truly intelligent AI must understand the intentionality behind human actions, not just recognize visual patterns. My interdisciplinary approach combines computer vision, generative AI, and biomedical applications, positioning me to tackle fundamental challenges in embodied AI where spatial-temporal understanding of human behavior is critical. Whether developing memory-based architectures for anomaly detection, real-time monitoring systems that construct 3D scene representations, or geometry-aware reconstruction models for medical imaging analysis, I'm consistently working toward AI systems that don't just process visual data, but comprehend the physics, intentionality, and temporal dynamics of how humans move through and interact with their world—creating AI that truly understands human behavior in 3D space.</p>
          </div>
        </div>
        <aside class="metrics-bar card">
          <div><span class="metric-value">4.0</span><span class="metric-label">GPA</span></div>
          <div><span class="metric-value">3</span><span class="metric-label">Publications</span></div>
          <div><span class="metric-value">6+</span><span class="metric-label">Research Projects</span></div>
          <div><span class="metric-value">3</span><span class="metric-label">Years Exp</span></div>
        </aside>
      </div>
    </section>

    <!-- PROFILE -->
    <section id="about" class="section about-section">
      <div class="container profile-grid">
        <div class="education-card card">
          <h3>Master of Science in Computer Science</h3>
          <div class="edu-institution">Texas A&M University</div>
          <div class="edu-duration">Aug 2024 - Jun 2026 (Expected) | College Station, TX</div>
          <div class="edu-gpa">GPA: 4.0</div>
          <div class="edu-meta">
            <strong>Current Coursework:</strong> Deep Reinforcement Learning, Generative AI, Research<br>
            <strong>Completed:</strong> Vision Foundation Models, ML for 3D Visualization & Graphics, Machine Learning, Analysis of Algorithms
          </div>
          <div class="advisors-section">
            <strong>Advisors:</strong>
            Prof. Cheng Zhang (Egocentric Vision & 3D Scene Understanding),
            Dr. Reza Avazmohammadi (Medical Imaging & Cardiac Analysis)
          </div>
        </div>
        <div class="skills-matrix card">
          <h3>Skills</h3>
          <div>
            <strong>Domains:</strong> Embodied AI, Egocentric Vision, Human-Centric CV, 3D Scene Understanding, Diffusion Models, Medical Imaging, 3D Gaussian Splatting, Multi-view Tracking
          </div>
          <div>
            <strong>Frameworks:</strong> PyTorch, TensorFlow, Hugging Face<br>
            <strong>Programming:</strong> Python, C++, Rust<br>
            <strong>Tools:</strong> YOLO, ONNX, Open3D, Viser, Rerun.io, SMPL, OpenCV, Docker
          </div>
        </div>
      </div>
    </section>

    <!-- RESEARCH -->
    <section id="research" class="section research-section">
      <div class="container research-cards-grid">
        <div class="card">
          <h3>Egocentric Fine-Grained Object Interaction Trajectory & Affordance Prediction</h3>
          <div>May 2025 - Present | Advisor: Prof. Cheng Zhang</div>
          <div class="desc">Developing egocentric view driven end-to-end framework for predicting temporal future location prediction in 3D voxel scene representation along with precise & fine-grained affordance trajectory to interact with objects. This research enables long-horizon task planning in complex 3D environments.</div>
          <div class="tags">Egocentric Vision, Affordance Prediction, 3D Scene Understanding, Long-horizon Planning</div>
        </div>
        <div class="card">
          <h3>HG-SCRUB: Human-Gaussian Scene Representation with Unified Baking</h3>
          <div>Jan 2025 - May 2025</div>
          <div class="desc">Developed framework for human and scene decomposition in separate representations given sparse image frames from video. Successfully learned material and physical components (Normals, Albedo, Roughness & Metallic) with explicit normal learning using depth derivation and baking for human gaussians to be relighted based on scene's environment map.</div>
          <div class="tags">3D Gaussian Splatting, Material Decomposition, Neural Rendering, Human-Scene Separation</div>
        </div>
      </div>
    </section>

    <!-- PUBLICATIONS -->
    <section id="publications" class="section publications-section">
      <div class="container publications-grid">
        <div class="card">
          <div class="paper-title">ATAC-Net: Zoomed view works better for Anomaly Detection</div>
          <div>ICIP 2024 | Shaurya Gupta, <strong>Neil Gautam</strong>, Anurag Malyala</div>
          <div class="paper-desc">Novel memory-based semi-supervised architecture achieving <strong>98% catch rate</strong> with <strong>&lt;1% false positive</strong> rate through attention-guided cropping for document tampering detection. Selected for <strong>spotlight presentation (top 5%)</strong>.</div>
          <div><a href="#">Paper</a> &bull; <a href="#">Code</a></div>
        </div>
        <div class="card">
          <div class="paper-title">Obscenity Detection in Videos through Sequential ConvNet Pipeline Classifier</div>
          <div>IEEE TCDS | <strong>Neil Gautam</strong>, Dr. Dinesh Kumar Vishwakarma</div>
          <div class="paper-desc">Sequential ConvNet pipeline for automated video content moderation using temporal-spatial feature extraction and classification methodologies for real-time video analysis.</div>
          <div><a href="#">Paper</a></div>
        </div>
      </div>
    </section>

    <!-- EXPERIENCE -->
    <section id="experience" class="section experience-section">
      <div class="container experience-grid">
        <div class="card">
          <div class="role-title">Research Intern, AI4Arch</div>
          <div>Texas A&M University, June 2025 - August 2025</div>
          <div class="desc">Advanced multi-view detection and tracking systems for real-time healthcare monitoring. Pioneered real-time people tracking and re-identification in Intensive Healthcare Units. Advisor: Dr. Cheng Zhang & Dr. Roxana Jafari.</div>
          <ul>
            <li>Multi-view detection & tracking in real-time with PyTorch, YOLOv11, ONNX, SMPL</li>
            <li>Constructed observed 3D Scene with objects using multi-view input</li>
            <li>Automated health code compliance monitoring</li>
          </ul>
        </div>
        <!-- Add other experience cards here in the same style -->
      </div>
    </section>
  </main>
</body>
</html>
