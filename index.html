<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neil Gautam | AI Researcher</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Animated Background -->
    <div class="neural-bg">
        <canvas id="neural-canvas"></canvas>
    </div>

    <!-- Top Navigation Bar -->
    <nav class="top-nav">
        <div class="nav-container">
            <div class="nav-brand">
                <span class="brand-text">neil_gautam@ai_researcher</span>
                <span class="terminal-cursor">‚ñà</span>
            </div>
            <div class="nav-items">
                <a href="#hero" class="nav-item active" data-section="hero">
                    <span class="nav-icon">‚óè</span>
                    <span class="nav-text">INIT</span>
                </a>
                <a href="#about" class="nav-item" data-section="about">
                    <span class="nav-icon">‚óÜ</span>
                    <span class="nav-text">PROFILE</span>
                </a>
                <a href="#research" class="nav-item" data-section="research">
                    <span class="nav-icon">‚ñ≤</span>
                    <span class="nav-text">RESEARCH</span>
                </a>
                <a href="#publications" class="nav-item" data-section="publications">
                    <span class="nav-icon">‚ñ†</span>
                    <span class="nav-text">PAPERS</span>
                </a>
                <a href="#experience" class="nav-item" data-section="experience">
                    <span class="nav-icon">‚óâ</span>
                    <span class="nav-text">EXPERIENCE</span>
                </a>
            </div>
        </div>
    </nav>

    <!-- Enhanced Hero Section with Larger Terminal -->
    <section id="hero" class="section hero-section">
        <div class="hero-container">
            <div class="terminal-window-large">
                <div class="terminal-header">
                    <div class="terminal-controls">
                        <span class="control red"></span>
                        <span class="control yellow"></span>
                        <span class="control green"></span>
                    </div>
                    <div class="terminal-title">neil_gautam@ai_researcher:~$</div>
                </div>
                <div class="terminal-body">
                    <div class="terminal-main">
                        <div class="typing-text">
                            <span class="prompt">$</span> <span class="command">whoami</span>
                            <div class="output">
                                <div class="name-display">NEIL GAUTAM</div>
                                <div class="role-text">AI Researcher</div>
                                <div class="specialization">[ Embodied AI ‚Ä¢ Human-Centric Computer Vision ‚Ä¢ Egocentric Systems ]</div>
                            </div>
                        </div>
                        <div class="typing-text delay-1">
                            <span class="prompt">$</span> <span class="command">cat research_focus.txt</span>
                            <div class="output research-focus-long">
                                <p>My research interest lies at the intersection of embodied AI and human-centric computer vision, where I develop intelligent systems that can understand and predict how humans perceive, navigate, and interact with complex 3D environments. Drawing from my experience building production-scale vision systems and generative models, I've evolved toward addressing fundamental questions about human behavior prediction and spatial scene understanding.</p>
                                
                                <p>My current focus centers on egocentric vision and affordance modeling‚Äîdeveloping frameworks that move beyond traditional computer vision to predict how humans will interact with their environment over time, bridging 2D visual perception with 3D spatial reasoning through advanced generative architectures, particularly diffusion models applied across diverse domains from healthcare to robust vision systems.</p>
                                
                                <p>What drives my research is the conviction that truly intelligent AI must understand the intentionality behind human actions, not just recognize visual patterns. My interdisciplinary approach combines computer vision, generative AI, and biomedical applications, positioning me to tackle fundamental challenges in embodied AI where spatial-temporal understanding of human behavior is critical. Whether developing memory-based architectures for anomaly detection, real-time monitoring systems that construct 3D scene representations, or geometry-aware reconstruction models for medical imaging analysis, I'm consistently working toward AI systems that don't just process visual data, but comprehend the physics, intentionality, and temporal dynamics of how humans move through and interact with their world‚Äîcreating AI that truly understands human behavior in 3D space.</p>
                            </div>
                        </div>
                        <div class="typing-text delay-2">
                            <span class="prompt">$</span> <span class="command">echo $CONTACT</span>
                            <div class="output contact-links">
                                <a href="mailto:neilgautam.24@tamu.edu" class="contact-link">
                                    <span class="link-icon">@</span> neilgautam.24@tamu.edu
                                </a>
                                <a href="https://www.linkedin.com/in/neil-gautam-a928971a5/" target="_blank" class="contact-link">
                                    <span class="link-icon">IN</span> LinkedIn
                                </a>
                                <a href="https://github.com/neilgautam24-tamu" target="_blank" class="contact-link">
                                    <span class="link-icon">GH</span> GitHub
                                </a>
                                <a href="https://scholar.google.com/citations?user=CUISVw8AAAAJ&hl=en" target="_blank" class="contact-link">
                                    <span class="link-icon">GS</span> Scholar
                                </a>
                            </div>
                        </div>
                        <div class="cursor-line">
                            <span class="prompt">$</span> <span class="blinking-cursor">‚ñà</span>
                        </div>
                    </div>
                    
                    <!-- Integrated Performance Metrics -->
                    <div class="terminal-sidebar">
                        <div class="metrics-header">PERFORMANCE_METRICS</div>
                        <div class="metrics-grid">
                            <div class="metric-item">
                                <div class="metric-number">4.0</div>
                                <div class="metric-label">GPA</div>
                                <div class="metric-bar"><div class="metric-fill" style="width: 100%"></div></div>
                            </div>
                            <div class="metric-item">
                                <div class="metric-number">3</div>
                                <div class="metric-label">PUBLICATIONS</div>
                                <div class="metric-bar"><div class="metric-fill" style="width: 85%"></div></div>
                            </div>
                            <div class="metric-item">
                                <div class="metric-number">6+</div>
                                <div class="metric-label">RESEARCH PROJECTS</div>
                                <div class="metric-bar"><div class="metric-fill" style="width: 95%"></div></div>
                            </div>
                            <div class="metric-item">
                                <div class="metric-number">3</div>
                                <div class="metric-label">YEARS EXP</div>
                                <div class="metric-bar"><div class="metric-fill" style="width: 80%"></div></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Enhanced About Section with Education -->
    <section id="about" class="section about-section">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title">NEURAL_PROFILE.exe</h2>
                <div class="section-subtitle">Pioneering Embodied AI & Human-Centric Vision</div>
            </div>
            
            <div class="profile-layout">
                <div class="education-main">
                    <div class="education-card current">
                        <div class="education-header">
                            <div class="edu-icon">üéì</div>
                            <div class="edu-info">
                                <h3>Master of Science in Computer Science</h3>
                                <div class="edu-school">Texas A&M University</div>
                                <div class="edu-duration">Aug 2024 - Jun 2026 (Expected)</div>
                                <div class="edu-location">College Station, TX</div>
                            </div>
                            <div class="edu-gpa perfect">4.0</div>
                        </div>
                        
                        <div class="coursework-section">
                            <div class="course-category">
                                <div class="category-label">Current Coursework (Fall 2025)</div>
                                <div class="course-pills">
                                    <span class="course-pill active">Deep Reinforcement Learning</span>
                                    <span class="course-pill active">Generative AI</span>
                                    <span class="course-pill active">Research</span>
                                </div>
                            </div>
                            
                            <div class="course-category">
                                <div class="category-label">Completed Coursework</div>
                                <div class="course-pills">
                                    <span class="course-pill completed">Vision Foundation Models</span>
                                    <span class="course-pill completed">ML for 3D Visualization & Graphics</span>
                                    <span class="course-pill completed">Machine Learning</span>
                                    <span class="course-pill completed">Analysis of Algorithms</span>
                                </div>
                            </div>
                        </div>

                        <div class="advisors-section">
                            <div class="advisors-header">[RESEARCH_ADVISORS]</div>
                            <div class="advisor-cards">
                                <div class="advisor-card">
                                    <div class="advisor-icon">üß†</div>
                                    <div class="advisor-info">
                                        <h4><a href="https://czhang0528.github.io/" target="_blank">Prof. Cheng Zhang</a></h4>
                                        <p>Main Advisor - Egocentric Vision & 3D Scene Understanding</p>
                                    </div>
                                </div>
                                <div class="advisor-card">
                                    <div class="advisor-icon">ü´Ä</div>
                                    <div class="advisor-info">
                                        <h4><a href="https://engineering.tamu.edu/biomedical/profiles/avazmohammadi-reza.html" target="_blank">Dr. Reza Avazmohammadi</a></h4>
                                        <p>Co-Advisor - Medical Imaging & Cardiac Analysis</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="education-card">
                        <div class="education-header">
                            <div class="edu-icon">üéØ</div>
                            <div class="edu-info">
                                <h3>Bachelor of Technology (B.Tech)</h3>
                                <div class="edu-school">Delhi Technological University</div>
                                <div class="edu-duration">Aug 2018 - May 2022</div>
                                <div class="edu-location">Delhi, India</div>
                            </div>
                            <div class="edu-gpa high">8.6</div>
                        </div>
                        
                        <div class="edu-details">
                            <p><strong>Major:</strong> Information Technology</p>
                        </div>

                        <div class="advisors-section">
                            <div class="advisors-header">[THESIS_ADVISOR]</div>
                            <div class="advisor-cards">
                                <div class="advisor-card">
                                    <div class="advisor-icon">üë®‚Äçüè´</div>
                                    <div class="advisor-info">
                                        <h4>Dr. Dinesh Kumar Vishwakarma</h4>
                                        <p>Thesis Supervisor - Computer Vision & Deep Learning</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                
                <div class="skills-matrix">
                    <div class="matrix-header">SKILL_MATRIX.exe</div>
                    <div class="skill-categories">
                        <div class="skill-category">
                            <div class="category-name">[RESEARCH_DOMAINS]</div>
                            <div class="skill-tags">
                                <span class="skill-tag">Embodied AI</span>
                                <span class="skill-tag">Egocentric Vision</span>
                                <span class="skill-tag">Human-Centric CV</span>
                                <span class="skill-tag">3D Scene Understanding</span>
                                <span class="skill-tag">Diffusion Models</span>
                                <span class="skill-tag">Medical Imaging</span>
                                <span class="skill-tag">3D Gaussian Splatting</span>
                                <span class="skill-tag">Multi-view Tracking</span>
                            </div>
                        </div>
                        
                        <div class="skill-category">
                            <div class="category-name">[ML_FRAMEWORKS]</div>
                            <div class="skill-progress">
                                <div class="progress-item">
                                    <span class="tech-name">PyTorch</span>
                                    <div class="progress-bar"><div class="progress-fill" data-width="98%"></div></div>
                                    <span class="progress-percent">98%</span>
                                </div>
                                <div class="progress-item">
                                    <span class="tech-name">TensorFlow</span>
                                    <div class="progress-bar"><div class="progress-fill" data-width="85%"></div></div>
                                    <span class="progress-percent">85%</span>
                                </div>
                                <div class="progress-item">
                                    <span class="tech-name">Hugging Face</span>
                                    <div class="progress-bar"><div class="progress-fill" data-width="92%"></div></div>
                                    <span class="progress-percent">92%</span>
                                </div>
                            </div>
                        </div>
                        
                        <div class="skill-category">
                            <div class="category-name">[PROGRAMMING]</div>
                            <div class="skill-progress">
                                <div class="progress-item">
                                    <span class="tech-name">Python</span>
                                    <div class="progress-bar"><div class="progress-fill" data-width="98%"></div></div>
                                    <span class="progress-percent">98%</span>
                                </div>
                                <div class="progress-item">
                                    <span class="tech-name">C++</span>
                                    <div class="progress-bar"><div class="progress-fill" data-width="82%"></div></div>
                                    <span class="progress-percent">82%</span>
                                </div>
                                <div class="progress-item">
                                    <span class="tech-name">Rust</span>
                                    <div class="progress-bar"><div class="progress-fill" data-width="75%"></div></div>
                                    <span class="progress-percent">75%</span>
                                </div>
                            </div>
                        </div>
                        
                        <div class="skill-category">
                            <div class="category-name">[SPECIALIZED_TOOLS]</div>
                            <div class="skill-tags">
                                <span class="skill-tag">YOLO</span>
                                <span class="skill-tag">ONNX</span>
                                <span class="skill-tag">Open3D</span>
                                <span class="skill-tag">Viser</span>
                                <span class="skill-tag">Rerun.io</span>
                                <span class="skill-tag">SMPL</span>
                                <span class="skill-tag">OpenCV</span>
                                <span class="skill-tag">Docker</span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Redesigned Research Timeline -->
    <section id="research" class="section research-section">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title">RESEARCH_TIMELINE.exe</h2>
                <div class="section-subtitle">Neural Networks Meet Reality</div>
            </div>
            
            <div class="research-grid-layout">
                <div class="research-card featured current">
                    <div class="card-ribbon">ACTIVE</div>
                    <div class="research-header">
                        <div class="research-icon">ü§ñ</div>
                        <div class="research-meta">
                            <h3>Egocentric Fine-Grained Object Interaction Trajectory & Affordance Prediction</h3>
                            <div class="research-details">
                                <span class="research-date">May 2025 - Present</span>
                                <span class="research-advisor">Advisor: <a href="https://czhang0528.github.io/" target="_blank">Prof. Cheng Zhang</a></span>
                            </div>
                        </div>
                    </div>
                    <p class="research-desc">
                        Developing egocentric view driven end-to-end framework for predicting temporal future location prediction 
                        in 3D voxel scene representation along with precise & fine-grained affordance trajectory to interact with objects. 
                        This research enables long-horizon task planning in complex 3D environments.
                    </p>
                    <div class="research-tags">
                        <span class="tag primary">Egocentric Vision</span>
                        <span class="tag">Affordance Prediction</span>
                        <span class="tag">3D Scene Understanding</span>
                        <span class="tag">Long-horizon Planning</span>
                    </div>
                </div>

                <div class="research-card">
                    <div class="card-ribbon completed">COMPLETED</div>
                    <div class="research-header">
                        <div class="research-icon">üéØ</div>
                        <div class="research-meta">
                            <h3>HG-SCRUB: Human-Gaussian Scene Representation with Unified Baking</h3>
                            <div class="research-details">
                                <span class="research-date">Jan 2025 - May 2025</span>
                            </div>
                        </div>
                    </div>
                    <p class="research-desc">
                        Developed framework for human and scene decomposition in separate representations given sparse image frames from video. 
                        Successfully learned material and physical components (Normals, Albedo, Roughness & Metallic) with explicit normal 
                        learning using depth derivation and baking for human gaussians to be relighted based on scene's environment map.
                    </p>
                    <div class="research-tags">
                        <span class="tag primary">3D Gaussian Splatting</span>
                        <span class="tag">Material Decomposition</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Human-Scene Separation</span>
                    </div>
                </div>

                <div class="research-card">
                    <div class="card-ribbon completed">COMPLETED</div>
                    <div class="research-header">
                        <div class="research-icon">üé¨</div>
                        <div class="research-meta">
                            <h3>Free-Generation: Training-Free Video Diffusion Control</h3>
                            <div class="research-details">
                                <span class="research-date">Aug 2024 - Dec 2024</span>
                                <span class="research-advisor">Advisor: <a href="https://czhang0528.github.io/" target="_blank">Prof. Cheng Zhang</a></span>
                            </div>
                        </div>
                    </div>
                    <p class="research-desc">
                        Conducted empirical research on Training-Free Trajectory Control Methods in Video Diffusion Models. 
                        Identified critical domain shifts in Attention Block Statistics and formulated innovative Statistical 
                        Post-Mask Normalization approaches for enhanced trajectory control performance.
                    </p>
                    <div class="research-tags">
                        <span class="tag primary">Video Diffusion</span>
                        <span class="tag">Trajectory Control</span>
                        <span class="tag">Attention Mechanisms</span>
                        <span class="tag">Training-Free Methods</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Publications Section -->
    <section id="publications" class="section publications-section">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title">PUBLICATIONS.db</h2>
                <div class="section-subtitle">Peer-Reviewed Research Output</div>
            </div>
            
            <div class="publications-grid">
                <div class="publication-card featured" data-abstract="The application of deep learning in visual anomaly detection has gained widespread popularity due to its potential use in quality control and manufacturing. Current standard methods are Unsupervised, where a clean dataset is utilised to detect deviations and flag anomalies during testing. However, incorporating a few samples when the type of anomalies is known beforehand can significantly enhance performance. Thus, we propose ATAC-Net, a framework that trains to detect anomalies from a minimal set of known prior anomalies. Furthermore, we introduce attention-guided cropping, which provides a closer view of suspect regions during the training phase. Our framework is a reliable and easy-to-understand system for detecting anomalies, and we substantiate its superiority to some of the current state-of-the-art techniques in a comparable setting.">
                    <div class="paper-badge spotlight">SPOTLIGHT</div>
                    <div class="paper-venue">ICIP 2024</div>
                    <h3 class="paper-title">ATAC-Net: Zoomed view works better for Anomaly Detection</h3>
                    <p class="paper-authors">Shaurya Gupta, <strong>Neil Gautam</strong>, Anurag Malyala</p>
                    <p class="paper-description">
                        Novel memory-based semi-supervised architecture achieving 98% catch rate with <1% false positive 
                        rate through attention-guided cropping for document tampering detection. Selected for spotlight presentation (top 5%).
                    </p>
                    <div class="paper-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10647702" target="_blank" class="paper-link">
                            <span class="link-icon">üìÑ</span> Paper
                        </a>
                        <a href="#" class="paper-link">
                            <span class="link-icon">üíª</span> Code
                        </a>
                    </div>
                    <div class="paper-metrics">
                        <div class="metric">
                            <span class="metric-value">98%</span>
                            <span class="metric-label">Catch Rate</span>
                        </div>
                        <div class="metric">
                            <span class="metric-value"><1%</span>
                            <span class="metric-label">False Positive</span>
                        </div>
                    </div>
                </div>
                
                <div class="publication-card">
                    <div class="paper-venue">IEEE TCDS</div>
                    <h3 class="paper-title">Obscenity Detection in Videos through Sequential ConvNet Pipeline Classifier</h3>
                    <p class="paper-authors"><strong>Neil Gautam</strong>, Dr. Dinesh Kumar Vishwakarma</p>
                    <p class="paper-description">
                        Sequential ConvNet pipeline for automated video content moderation using temporal-spatial 
                        feature extraction and classification methodologies for real-time video analysis.
                    </p>
                    <div class="paper-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/9733936" target="_blank" class="paper-link">
                            <span class="link-icon">üìÑ</span> Paper
                        </a>
                    </div>
                </div>
                
                <div class="publication-card">
                    <div class="paper-venue">IEEE ICCMC</div>
                    <h3 class="paper-title">Ensemble Learning Using Vision Transformer and CNN for Person ReID</h3>
                    <p class="paper-authors">Gupta, <strong>Neil Gautam</strong>, Dr. Vishwakarma</p>
                    <p class="paper-description">
                        Novel ensemble approach combining Vision Transformers and CNNs for improved person 
                        re-identification performance across challenging scenarios and diverse datasets.
                    </p>
                    <div class="paper-links">
                        <a href="https://ieeexplore.ieee.org/abstract/document/9753761" target="_blank" class="paper-link">
                            <span class="link-icon">üìÑ</span> Paper
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Compact Experience Timeline -->
    <section id="experience" class="section experience-section">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title">EXPERIENCE_LOG.sys</h2>
                <div class="section-subtitle">From Research Labs to Production Systems</div>
            </div>
            
            <div class="experience-timeline-compact">
                <div class="timeline-line-compact"></div>
                
                <!-- 2025 Current -->
                <div class="experience-item-compact current">
                    <div class="timeline-marker-compact current">
                        <div class="marker-pulse-compact"></div>
                    </div>
                    <div class="experience-box-compact research">
                        <div class="experience-status-compact current">CURRENT</div>
                        <div class="experience-header-compact">
                            <div class="role-icon-compact">ü§ñ</div>
                            <div class="role-details-compact">
                                <h3 class="role-title-compact">Research Intern, AI4Arch</h3>
                                <div class="role-company-compact">Texas A&M University</div>
                                <div class="role-period-compact">June 2025 - August 2025</div>
                            </div>
                        </div>
                        
                        <div class="advisors-compact">
                            <span class="advisor-label-compact">Advisors:</span>
                            <a href="https://czhang0528.github.io/" target="_blank">Dr. Cheng Zhang</a> & Dr. Roxana Jafari
                        </div>
                        
                        <div class="role-summary-compact">
                            Advanced multi-view detection and tracking systems for real-time healthcare monitoring solutions using cutting-edge computer vision techniques. Pioneered real-time people tracking and re-identification in Intensive Healthcare Units.
                        </div>
                        
                        <div class="technologies-compact">
                            <div class="tech-header-compact">Technologies:</div>
                            <div class="tech-list-compact">
                                <span class="tech-item-compact">PyTorch</span>
                                <span class="tech-item-compact">YOLOv11</span>
                                <span class="tech-item-compact">ONNX</span>
                                <span class="tech-item-compact">SMPL</span>
                            </div>
                        </div>
                        
                        <div class="achievements-compact">
                            <div class="achievements-header-compact">Key Contributions:</div>
                            <ul class="achievements-list-compact">
                                <li>Developed multi-view detection (YOLOv11) & tracking algorithm based on generative feature templates for tracking & re-identifying people in Intensive Healthcare Units in real-time</li>
                                <li>Constructed observed 3D Scene with objects using multi-view input for representing humans in 3D (SMPL) to flag harmful behavior according to health codes</li>
                                <li>Implemented SMPL-based 3D human representation for automated health code compliance monitoring</li>
                                <li>Advanced computer vision pipeline for healthcare applications with real-time processing capabilities</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- 2024-Present Ongoing -->
                <div class="experience-item-compact">
                    <div class="timeline-marker-compact ongoing"></div>
                    <div class="experience-box-compact research">
                        <div class="experience-status-compact ongoing">ONGOING</div>
                        <div class="experience-header-compact">
                            <div class="role-icon-compact">ü´Ä</div>
                            <div class="role-details-compact">
                                <h3 class="role-title-compact">Student Assistant, C2B Lab</h3>
                                <div class="role-company-compact">Texas A&M University</div>
                                <div class="role-period-compact">Sep 2024 - Present</div>
                            </div>
                        </div>
                        
                        <div class="advisors-compact">
                            <span class="advisor-label-compact">Advisor:</span>
                            <a href="https://engineering.tamu.edu/biomedical/profiles/avazmohammadi-reza.html" target="_blank">Dr. Reza Avazmohammadi</a>
                        </div>
                        
                        <div class="role-summary-compact">
                            Developing 4D heart motion tracking systems using diffusion models for non-invasive cardiac analysis. Pioneering advanced medical imaging applications through novel machine learning approaches.
                        </div>
                        
                        <div class="technologies-compact">
                            <div class="tech-header-compact">Technologies:</div>
                            <div class="tech-list-compact">
                                <span class="tech-item-compact">PyTorch</span>
                                <span class="tech-item-compact">Matlab</span>
                                <span class="tech-item-compact">Python</span>
                                <span class="tech-item-compact">Viser</span>
                            </div>
                        </div>
                        
                        <div class="achievements-compact">
                            <div class="achievements-header-compact">Key Contributions:</div>
                            <ul class="achievements-list-compact">
                                <li>Currently working on 4D Motion tracking of Heart by mapping 2D based motion to 3D using motion interpolation techniques</li>
                                <li>Devised novel Semi-Supervised Diffusion based Image Registration approach for achieving 2D motion tracking of Rat's Heart</li>
                                <li>Developed Geometry aware (2D slice prior) Diffusion based 3D Reconstruction Model for precise mesh generation of Rat's Heart</li>
                                <li>Advanced cardiac motion analysis from echo images for non-invasive disease prediction solutions</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- 2024 Industry - AfterShoot with Impact Metrics -->
                <div class="experience-item-compact">
                    <div class="timeline-marker-compact completed"></div>
                    <div class="experience-box-compact industry">
                        <div class="experience-status-compact completed">COMPLETED</div>
                        <div class="experience-header-compact">
                            <div class="role-icon-compact">üé®</div>
                            <div class="role-details-compact">
                                <h3 class="role-title-compact">Machine Learning Engineer-2</h3>
                                <div class="role-company-compact">AfterShoot</div>
                                <div class="role-period-compact">Nov 2023 - Jul 2024</div>
                            </div>
                        </div>
                        
                        <div class="advisors-compact">
                            <span class="advisor-label-compact">Manager:</span>
                            <a href="https://www.linkedin.com/in/nikhil-bartwal-b07b501a3/" target="_blank">Nikhil Bartwal</a>
                        </div>
                        
                        <div class="impact-summary-compact">
                            <div class="impact-item-compact">
                                <span class="impact-number-compact">40%</span>
                                <span class="impact-desc-compact">Workflow Acceleration</span>
                            </div>
                            <div class="impact-item-compact">
                                <span class="impact-number-compact">10h‚Üí3h</span>
                                <span class="impact-desc-compact">Processing Time</span>
                            </div>
                            <div class="impact-item-compact">
                                <span class="impact-number-compact">1000+</span>
                                <span class="impact-desc-compact">Photos Enhanced</span>
                            </div>
                        </div>
                        
                        <div class="role-summary-compact">
                            Spearheaded development of AI portrait enhancement and retouching systems, delivering production-grade features that revolutionized photo editing workflows for professional photographers.
                        </div>
                        
                        <div class="technologies-compact">
                            <div class="tech-header-compact">Technologies:</div>
                            <div class="tech-list-compact">
                                <span class="tech-item-compact">Python</span>
                                <span class="tech-item-compact">C++</span>
                                <span class="tech-item-compact">Rust</span>
                                <span class="tech-item-compact">RunPod</span>
                                <span class="tech-item-compact">GCP</span>
                                <span class="tech-item-compact">Azure</span>
                                <span class="tech-item-compact">PyTorch</span>
                                <span class="tech-item-compact">TensorFlow</span>
                            </div>
                        </div>
                        
                        <div class="achievements-compact">
                            <div class="achievements-header-compact">Key Contributions:</div>
                            <ul class="achievements-list-compact">
                                <li>Spearheaded development of AI portrait enhancement and retouching, delivering features such as teeth correction, whitening, skin retouching. This product reduced photoshop time from approximately <span class="highlight-metric">10s of hours to < 2-3 hours for around 1000 photos</span></li>
                                <li>Engineered and deployed U-Net based generative model that allowed generative teeth correction capabilities in portrait photos</li>
                                <li>Accelerated photo editing workflow by <span class="highlight-metric">40%</span> by designing lightweight models automating HSL filters in Lightroom Classic</li>
                                <li>Deployed production-scale computer vision systems serving thousands of professional photographers</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- 2023 Industry - Savart (removed impact metrics) -->
                <div class="experience-item-compact">
                    <div class="timeline-marker-compact completed"></div>
                    <div class="experience-box-compact industry">
                        <div class="experience-status-compact completed">COMPLETED</div>
                        <div class="experience-header-compact">
                            <div class="role-icon-compact">üìà</div>
                            <div class="role-details-compact">
                                <h3 class="role-title-compact">Machine Learning Engineer</h3>
                                <div class="role-company-compact">Savart</div>
                                <div class="role-period-compact">Jan 2023 - Oct 2023</div>
                            </div>
                        </div>
                        
                        <div class="advisors-compact">
                            <span class="advisor-label-compact">Manager:</span>
                            <a href="https://www.linkedin.com/in/suhruth-eedara-93858412b/" target="_blank">Suhruth Eedara</a>
                        </div>
                        
                        <div class="role-summary-compact">
                            Fine-tuned large language models on financial reports and developed advanced NLP pipelines, revolutionizing equity market analysis through automated document processing and intelligent information extraction.
                        </div>
                        
                        <div class="technologies-compact">
                            <div class="tech-header-compact">Technologies:</div>
                            <div class="tech-list-compact">
                                <span class="tech-item-compact">Python</span>
                                <span class="tech-item-compact">C++</span>
                                <span class="tech-item-compact">AWS</span>
                                <span class="tech-item-compact">GCP</span>
                                <span class="tech-item-compact">Azure</span>
                                <span class="tech-item-compact">PyTorch</span>
                            </div>
                        </div>
                        
                        <div class="achievements-compact">
                            <div class="achievements-header-compact">Key Contributions:</div>
                            <ul class="achievements-list-compact">
                                <li>Fine-tuned LLAMA-2 and Alpaca models on financial annual reports that boosted qualitative company profiling capabilities to be used in equity market analysis. Boosted the efficiency of Equity market research from <span class="highlight-metric">weeks to days</span></li>
                                <li>Engineered dynamic OCR pipeline & automated information extraction. Processing more than <span class="highlight-metric">50K documents</span> for corpus creation</li>
                                <li>Enhanced QA accuracy by <span class="highlight-metric">35%</span> through strategic optimization of reader models (RoBERTa, BART, XLM) on novel dataset</li>
                                <li>Developed scalable NLP systems for financial document analysis and automated reporting</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- 2022 Research Intern - HyperVerge (removed impact metrics) -->
                <div class="experience-item-compact">
                    <div class="timeline-marker-compact completed"></div>
                    <div class="experience-box-compact research-intern">
                        <div class="experience-status-compact spotlight">SPOTLIGHT</div>
                        <div class="experience-header-compact">
                            <div class="role-icon-compact">üîç</div>
                            <div class="role-details-compact">
                                <h3 class="role-title-compact">Deep Learning Research Intern</h3>
                                <div class="role-company-compact">HyperVerge Ltd</div>
                                <div class="role-period-compact">Jul 2022 - Dec 2022</div>
                            </div>
                        </div>
                        
                        <div class="advisors-compact">
                            <span class="advisor-label-compact">Supervisor:</span>
                            <a href="https://www.linkedin.com/in/hariprasad-p-s-162b0778/" target="_blank">Hariprasad P S</a>
                        </div>
                        
                        <div class="role-summary-compact">
                            Led R&D for industry-leading document tampering detection systems serving global markets. Developed novel anomaly detection architecture that achieved unprecedented accuracy rates and was selected for prestigious academic recognition.
                        </div>
                        
                        <div class="achievements-compact">
                            <div class="achievements-header-compact">Key Contributions:</div>
                            <ul class="achievements-list-compact">
                                <li>Spearheaded R&D for industry-leading ID tampering detection system serving <span class="highlight-metric">over 100 clients</span> with state-of-the-art tampering detection system</li>
                                <li>Developed tampering detection system that achieved an unprecedented <span class="highlight-metric">98% catch rate with less than 1% false positive rate</span></li>
                                <li>Proposed a novel anomaly detection approach (ATAC-Net) using memory-based semi-supervised architecture after experimenting with MemSeg, DeviationNet, WS-DAN to create precise tampering localization in documents across diverse document types</li>
                                <li>This work was selected among the <span class="highlight-metric">best 5%</span> and chosen for the spotlight presentation at ICIP 2024</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Abstract Tooltip -->
    <div id="abstract-tooltip" class="abstract-tooltip">
        <div class="tooltip-header">PAPER_ABSTRACT.txt</div>
        <div class="tooltip-content">
            <p id="tooltip-text"></p>
        </div>
    </div>

    <!-- Scroll Progress Indicator -->
    <div class="scroll-progress">
        <div class="progress-bar"></div>
    </div>

    <script src="script.js"></script>
</body>
</html>
